{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0.1\n",
      "0.15.2\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "\n",
    "print(torch.__version__)\n",
    "print(torchvision.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "# 设置 sys.argv 为默认值或仅包含脚本名称\n",
    "sys.argv = [\"\"]\n",
    "\n",
    "# 然后你可以安全地调用你的解析代码\n",
    "# args = parser.parse_args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from options.my_train_options import TrainOptions\n",
    "from data import create_dataset\n",
    "from models import create_model\n",
    "from util.visualizer import Visualizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------- Options ---------------\n",
      "               batch_size: 1                             \n",
      "                    beta1: 0.5                           \n",
      "          checkpoints_dir: ./checkpoints                 \n",
      "           continue_train: False                         \n",
      "                crop_size: 256                           \n",
      "                 dataroot: E:/UIE_larry/UIE_larry/my_practice/datasets/mydatasets/my_aligned_dataset\n",
      "             dataset_mode: aligned                       \n",
      "                direction: AtoB                          \n",
      "              display_env: main                          \n",
      "             display_freq: 400                           \n",
      "               display_id: -1                            \n",
      "            display_ncols: 4                             \n",
      "             display_port: 8097                          \n",
      "           display_server: http://localhost              \n",
      "          display_winsize: 256                           \n",
      "                    epoch: latest                        \n",
      "              epoch_count: 1                             \n",
      "                 gan_mode: lsgan                         \n",
      "                  gpu_ids: 0                             \n",
      "                init_gain: 0.02                          \n",
      "                init_type: normal                        \n",
      "                 input_nc: 3                             \n",
      "                  isTrain: True                          \t[default: None]\n",
      "                lambda_L1: 100.0                         \n",
      "                load_iter: 0                             \t[default: 0]\n",
      "                load_size: 286                           \n",
      "                       lr: 0.0002                        \n",
      "           lr_decay_iters: 50                            \n",
      "                lr_policy: linear                        \n",
      "         max_dataset_size: inf                           \n",
      "                    model: pix2pix                       \n",
      "                 n_epochs: 100                           \n",
      "           n_epochs_decay: 100                           \n",
      "               n_layers_D: 3                             \n",
      "                     name: unet_rgb_L2                   \n",
      "                      ndf: 64                            \n",
      "                     netD: basic                         \n",
      "                     netG: unet_256                      \n",
      "                      ngf: 64                            \n",
      "               no_dropout: False                         \n",
      "                  no_flip: False                         \n",
      "                  no_html: False                         \n",
      "                     norm: batch                         \n",
      "              num_threads: 4                             \n",
      "                output_nc: 3                             \n",
      "                    phase: train                         \n",
      "                pool_size: 0                             \n",
      "               preprocess: resize_and_crop               \n",
      "               print_freq: 100                           \n",
      "             save_by_iter: False                         \n",
      "          save_epoch_freq: 5                             \n",
      "         save_latest_freq: 5000                          \n",
      "           serial_batches: False                         \n",
      "                   suffix:                               \n",
      "         update_html_freq: 1000                          \n",
      "                use_wandb: False                         \n",
      "                  verbose: False                         \n",
      "       wandb_project_name: CycleGAN-and-pix2pix          \n",
      "----------------- End -------------------\n"
     ]
    }
   ],
   "source": [
    "opt = TrainOptions().parse()  # get training options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset [AlignedDataset] was created\n"
     ]
    }
   ],
   "source": [
    "dataset = create_dataset(\n",
    "    opt\n",
    ")  # create a dataset given opt.dataset_mode and other options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of training images = 3679\n"
     ]
    }
   ],
   "source": [
    "dataset_size = len(dataset)  # get the number of images in the dataset.\n",
    "print(\"The number of training images = %d\" % dataset_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initialize network with normal\n",
      "initialize network with normal\n",
      "model [Pix2PixModel] was created\n",
      "---------- Networks initialized -------------\n",
      "[Network G] Total number of parameters : 54.414 M\n",
      "[Network D] Total number of parameters : 2.769 M\n",
      "-----------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "model = create_model(opt)  # create a model given opt.model and other options\n",
    "model.setup(opt)  # regular setup: load and print networks; create schedulers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Pix2PixModel' object has no attribute 'train'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m()\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'Pix2PixModel' object has no attribute 'train'"
     ]
    }
   ],
   "source": [
    "model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'A': tensor([[[[-0.1216, -0.1216, -0.1294,  ..., -0.2784, -0.2784, -0.2784],\n",
       "           [-0.1294, -0.1294, -0.1373,  ..., -0.2784, -0.2784, -0.2784],\n",
       "           [-0.1294, -0.1294, -0.1373,  ..., -0.2784, -0.2784, -0.2784],\n",
       "           ...,\n",
       "           [-0.4510, -0.4196, -0.3490,  ..., -0.2314, -0.2314, -0.2235],\n",
       "           [-0.4118, -0.4039, -0.3490,  ..., -0.2392, -0.2392, -0.2314],\n",
       "           [-0.4118, -0.4275, -0.3804,  ..., -0.2392, -0.2471, -0.2392]],\n",
       " \n",
       "          [[ 0.5922,  0.5922,  0.5922,  ...,  0.5294,  0.5294,  0.5294],\n",
       "           [ 0.5843,  0.5843,  0.5843,  ...,  0.5294,  0.5294,  0.5294],\n",
       "           [ 0.5843,  0.5843,  0.5843,  ...,  0.5294,  0.5294,  0.5294],\n",
       "           ...,\n",
       "           [ 0.0510,  0.0824,  0.1608,  ...,  0.4588,  0.4588,  0.4667],\n",
       "           [ 0.0667,  0.0902,  0.1529,  ...,  0.4510,  0.4510,  0.4588],\n",
       "           [ 0.0588,  0.0588,  0.1216,  ...,  0.4510,  0.4431,  0.4510]],\n",
       " \n",
       "          [[-0.3412, -0.3412, -0.3412,  ..., -0.3725, -0.3725, -0.3725],\n",
       "           [-0.3490, -0.3490, -0.3490,  ..., -0.3725, -0.3725, -0.3725],\n",
       "           [-0.3490, -0.3490, -0.3490,  ..., -0.3725, -0.3725, -0.3725],\n",
       "           ...,\n",
       "           [-0.7098, -0.6784, -0.6235,  ..., -0.4824, -0.4824, -0.4745],\n",
       "           [-0.6784, -0.6706, -0.6078,  ..., -0.4902, -0.4902, -0.4824],\n",
       "           [-0.6784, -0.6863, -0.6314,  ..., -0.4902, -0.4980, -0.4902]]]]),\n",
       " 'B': tensor([[[[ 0.1922,  0.1922,  0.1843,  ...,  0.0196,  0.0196,  0.0275],\n",
       "           [ 0.1843,  0.1765,  0.1765,  ...,  0.0196,  0.0275,  0.0275],\n",
       "           [ 0.1686,  0.1608,  0.1686,  ...,  0.0275,  0.0275,  0.0275],\n",
       "           ...,\n",
       "           [-0.5137, -0.4902, -0.3647,  ..., -0.0118, -0.0353, -0.0353],\n",
       "           [-0.4980, -0.4667, -0.3804,  ..., -0.0196, -0.0431, -0.0431],\n",
       "           [-0.4353, -0.4275, -0.4039,  ..., -0.0039, -0.0353, -0.0431]],\n",
       " \n",
       "          [[ 0.2157,  0.2078,  0.2157,  ...,  0.0980,  0.0980,  0.1059],\n",
       "           [ 0.2078,  0.2000,  0.2078,  ...,  0.0980,  0.1059,  0.1059],\n",
       "           [ 0.1922,  0.1843,  0.2078,  ...,  0.1059,  0.1059,  0.1059],\n",
       "           ...,\n",
       "           [-0.6706, -0.6471, -0.5137,  ...,  0.0039, -0.0196, -0.0196],\n",
       "           [-0.6627, -0.6235, -0.5451,  ..., -0.0039, -0.0275, -0.0275],\n",
       "           [-0.6078, -0.5922, -0.5608,  ...,  0.0118, -0.0196, -0.0275]],\n",
       " \n",
       "          [[ 0.3569,  0.3569,  0.3647,  ...,  0.2941,  0.2941,  0.3020],\n",
       "           [ 0.3412,  0.3333,  0.3569,  ...,  0.2941,  0.3020,  0.3020],\n",
       "           [ 0.3255,  0.3176,  0.3412,  ...,  0.3020,  0.3020,  0.3020],\n",
       "           ...,\n",
       "           [-0.6784, -0.6549, -0.5294,  ..., -0.0039, -0.0275, -0.0275],\n",
       "           [-0.6706, -0.6392, -0.5451,  ..., -0.0118, -0.0353, -0.0353],\n",
       "           [-0.6235, -0.6078, -0.5686,  ...,  0.0039, -0.0275, -0.0353]]]]),\n",
       " 'A_paths': ['E:/UIE_larry/UIE_larry/my_practice/datasets/mydatasets/my_aligned_dataset\\\\train\\\\1582.jpg'],\n",
       " 'B_paths': ['E:/UIE_larry/UIE_larry/my_practice/datasets/mydatasets/my_aligned_dataset\\\\train\\\\1582.jpg']}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 假设 'dataset' 是一个 PyTorch DataLoader 实例\n",
    "data_iterator = iter(dataset)\n",
    "\n",
    "# 使用 next() 方法获取一批数据\n",
    "one_batch = next(data_iterator)\n",
    "\n",
    "one_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 提取输入图像（A）和目标图像（B）的张量\n",
    "A_images = one_batch[\"A\"]\n",
    "B_images = one_batch[\"B\"]\n",
    "\n",
    "# 如果存在，提取图像路径（A_paths 和 B_paths）\n",
    "A_paths = one_batch.get(\n",
    "    \"A_paths\", None\n",
    ")  # 使用 get 方法可以避免 KeyError，如果键不存在，返回 None\n",
    "B_paths = one_batch.get(\"B_paths\", None)\n",
    "\n",
    "# 现在你可以使用 A_images 和 B_images 进行模型训练或测试\n",
    "# A_paths 和 B_paths 可以用于追踪图像的来源或进行其他的记录工作"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3, 256, 256])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A_images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"General-purpose training script for image-to-image translation.\n",
    "\n",
    "This script works for various models (with option '--model': e.g., pix2pix, cyclegan, colorization) and\n",
    "different datasets (with option '--dataset_mode': e.g., aligned, unaligned, single, colorization).\n",
    "You need to specify the dataset ('--dataroot'), experiment name ('--name'), and model ('--model').\n",
    "\n",
    "It first creates model, dataset, and visualizer given the option.\n",
    "It then does standard network training. During the training, it also visualize/save the images, print/save the loss plot, and save models.\n",
    "The script supports continue/resume training. Use '--continue_train' to resume your previous training.\n",
    "\n",
    "Example:\n",
    "    Train a CycleGAN model:\n",
    "        python train.py --dataroot ./datasets/maps --name maps_cyclegan --model cycle_gan\n",
    "    Train a pix2pix model:\n",
    "        python train.py --dataroot ./datasets/facades --name facades_pix2pix --model pix2pix --direction BtoA\n",
    "\n",
    "See options/base_options.py and options/train_options.py for more training options.\n",
    "See training and test tips at: https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/blob/master/docs/tips.md\n",
    "See frequently asked questions at: https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/blob/master/docs/qa.md\n",
    "\"\"\"\n",
    "\n",
    "import time\n",
    "from options.my_train_options import TrainOptions\n",
    "from data import create_dataset\n",
    "from models import create_model\n",
    "from util.visualizer import Visualizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "# 设置 sys.argv 为默认值或仅包含脚本名称\n",
    "sys.argv = [\"\"]\n",
    "\n",
    "# 然后你可以安全地调用你的解析代码\n",
    "# args = parser.parse_args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------- Options ---------------\n",
      "               batch_size: 1                             \n",
      "                    beta1: 0.5                           \n",
      "          checkpoints_dir: ./checkpoints                 \n",
      "           continue_train: False                         \n",
      "                crop_size: 256                           \n",
      "                 dataroot: E:/UIE_larry/UIE_larry/my_practice/datasets/mydatasets/my_aligned_dataset\n",
      "             dataset_mode: aligned                       \n",
      "                direction: AtoB                          \n",
      "              display_env: main                          \n",
      "             display_freq: 400                           \n",
      "               display_id: -1                            \n",
      "            display_ncols: 4                             \n",
      "             display_port: 8097                          \n",
      "           display_server: http://localhost              \n",
      "          display_winsize: 256                           \n",
      "                    epoch: latest                        \n",
      "              epoch_count: 1                             \n",
      "                 gan_mode: lsgan                         \n",
      "                  gpu_ids: 0                             \n",
      "                init_gain: 0.02                          \n",
      "                init_type: normal                        \n",
      "                 input_nc: 3                             \n",
      "                  isTrain: True                          \t[default: None]\n",
      "                lambda_L1: 100.0                         \n",
      "                load_iter: 0                             \t[default: 0]\n",
      "                load_size: 286                           \n",
      "                       lr: 0.0002                        \n",
      "           lr_decay_iters: 50                            \n",
      "                lr_policy: linear                        \n",
      "         max_dataset_size: inf                           \n",
      "                    model: pix2pix                       \n",
      "                 n_epochs: 100                           \n",
      "           n_epochs_decay: 100                           \n",
      "               n_layers_D: 3                             \n",
      "                     name: unet_rgb_L2                   \n",
      "                      ndf: 64                            \n",
      "                     netD: basic                         \n",
      "                     netG: unet_256                      \n",
      "                      ngf: 64                            \n",
      "               no_dropout: False                         \n",
      "                  no_flip: False                         \n",
      "                  no_html: False                         \n",
      "                     norm: batch                         \n",
      "              num_threads: 4                             \n",
      "                output_nc: 3                             \n",
      "                    phase: train                         \n",
      "                pool_size: 0                             \n",
      "               preprocess: resize_and_crop               \n",
      "               print_freq: 100                           \n",
      "             save_by_iter: False                         \n",
      "          save_epoch_freq: 5                             \n",
      "         save_latest_freq: 5000                          \n",
      "           serial_batches: False                         \n",
      "                   suffix:                               \n",
      "         update_html_freq: 1000                          \n",
      "                use_wandb: False                         \n",
      "                  verbose: False                         \n",
      "       wandb_project_name: CycleGAN-and-pix2pix          \n",
      "----------------- End -------------------\n"
     ]
    }
   ],
   "source": [
    "opt = TrainOptions().parse()  # get training options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "writer = SummaryWriter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------- Options ---------------\n",
      "               batch_size: 1                             \n",
      "                    beta1: 0.5                           \n",
      "          checkpoints_dir: ./checkpoints                 \n",
      "           continue_train: False                         \n",
      "                crop_size: 256                           \n",
      "                 dataroot: E:/UIE_larry/UIE_larry/my_practice/datasets/mydatasets/my_aligned_dataset\n",
      "             dataset_mode: aligned                       \n",
      "                direction: AtoB                          \n",
      "              display_env: main                          \n",
      "             display_freq: 400                           \n",
      "               display_id: -1                            \n",
      "            display_ncols: 4                             \n",
      "             display_port: 8097                          \n",
      "           display_server: http://localhost              \n",
      "          display_winsize: 256                           \n",
      "                    epoch: latest                        \n",
      "              epoch_count: 1                             \n",
      "                 gan_mode: lsgan                         \n",
      "                  gpu_ids: 0                             \n",
      "                init_gain: 0.02                          \n",
      "                init_type: normal                        \n",
      "                 input_nc: 3                             \n",
      "                  isTrain: True                          \t[default: None]\n",
      "                lambda_L1: 100.0                         \n",
      "                load_iter: 0                             \t[default: 0]\n",
      "                load_size: 286                           \n",
      "                       lr: 0.0002                        \n",
      "           lr_decay_iters: 50                            \n",
      "                lr_policy: linear                        \n",
      "         max_dataset_size: inf                           \n",
      "                    model: pix2pix                       \n",
      "                 n_epochs: 100                           \n",
      "           n_epochs_decay: 100                           \n",
      "               n_layers_D: 3                             \n",
      "                     name: unet_rgb_L2                   \n",
      "                      ndf: 64                            \n",
      "                     netD: basic                         \n",
      "                     netG: unet_256                      \n",
      "                      ngf: 64                            \n",
      "               no_dropout: False                         \n",
      "                  no_flip: False                         \n",
      "                  no_html: False                         \n",
      "                     norm: batch                         \n",
      "              num_threads: 4                             \n",
      "                output_nc: 3                             \n",
      "                    phase: train                         \n",
      "                pool_size: 0                             \n",
      "               preprocess: resize_and_crop               \n",
      "               print_freq: 100                           \n",
      "             save_by_iter: False                         \n",
      "          save_epoch_freq: 5                             \n",
      "         save_latest_freq: 5000                          \n",
      "           serial_batches: False                         \n",
      "                   suffix:                               \n",
      "         update_html_freq: 1000                          \n",
      "                use_wandb: False                         \n",
      "                  verbose: False                         \n",
      "       wandb_project_name: CycleGAN-and-pix2pix          \n",
      "----------------- End -------------------\n",
      "dataset [AlignedDataset] was created\n",
      "The number of training images = 3679\n",
      "initialize network with normal\n",
      "initialize network with normal\n",
      "model [Pix2PixModel] was created\n",
      "---------- Networks initialized -------------\n",
      "[Network G] Total number of parameters : 54.414 M\n",
      "[Network D] Total number of parameters : 2.769 M\n",
      "-----------------------------------------------\n",
      "learning rate 0.0002000 -> 0.0002000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\anaconda3\\envs\\inpainting-basic\\Lib\\site-packages\\torch\\optim\\lr_scheduler.py:139: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 30\u001b[0m\n\u001b[0;32m     28\u001b[0m total_iters \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m opt\u001b[38;5;241m.\u001b[39mbatch_size\n\u001b[0;32m     29\u001b[0m epoch_iter \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m opt\u001b[38;5;241m.\u001b[39mbatch_size\n\u001b[1;32m---> 30\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mset_input\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# unpack data from dataset and apply preprocessing\u001b[39;00m\n\u001b[0;32m     31\u001b[0m model\u001b[38;5;241m.\u001b[39moptimize_parameters()  \u001b[38;5;66;03m# calculate loss functions, get gradients, update network weights\u001b[39;00m\n\u001b[0;32m     33\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m     34\u001b[0m     total_iters \u001b[38;5;241m%\u001b[39m opt\u001b[38;5;241m.\u001b[39mdisplay_freq \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m     35\u001b[0m ):  \u001b[38;5;66;03m# display images on visdom and save images to a HTML file\u001b[39;00m\n",
      "File \u001b[1;32me:\\UIE_larry\\UIE_larry\\my_practice\\unet_gan_L2\\master_thesis\\models\\pix2pix_model.py:86\u001b[0m, in \u001b[0;36mPix2PixModel.set_input\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m     78\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Unpack input data from the dataloader and perform necessary pre-processing steps.\u001b[39;00m\n\u001b[0;32m     79\u001b[0m \n\u001b[0;32m     80\u001b[0m \u001b[38;5;124;03mParameters:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     83\u001b[0m \u001b[38;5;124;03mThe option 'direction' can be used to swap images in domain A and domain B.\u001b[39;00m\n\u001b[0;32m     84\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     85\u001b[0m AtoB \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mopt\u001b[38;5;241m.\u001b[39mdirection \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAtoB\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m---> 86\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreal_A \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43minput\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mA\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mAtoB\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mB\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     87\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreal_B \u001b[38;5;241m=\u001b[39m \u001b[38;5;28minput\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mB\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m AtoB \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mA\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m     88\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mimage_paths \u001b[38;5;241m=\u001b[39m \u001b[38;5;28minput\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mA_paths\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m AtoB \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mB_paths\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "opt = TrainOptions().parse()  # get training options\n",
    "dataset = create_dataset(\n",
    "    opt\n",
    ")  # create a dataset given opt.dataset_mode and other options\n",
    "dataset_size = len(dataset)  # get the number of images in the dataset.\n",
    "print(\"The number of training images = %d\" % dataset_size)\n",
    "\n",
    "model = create_model(opt)  # create a model given opt.model and other options\n",
    "model.setup(opt)  # regular setup: load and print networks; create schedulers\n",
    "visualizer = Visualizer(opt)  # create a visualizer that display/save images and plots\n",
    "total_iters = 0  # the total number of training iterations\n",
    "\n",
    "for epoch in range(\n",
    "    opt.epoch_count, opt.n_epochs + opt.n_epochs_decay + 1\n",
    "):  # outer loop for different epochs; we save the model by <epoch_count>, <epoch_count>+<save_latest_freq>\n",
    "    model.train()\n",
    "    epoch_start_time = time.time()  # timer for entire epoch\n",
    "    iter_data_time = time.time()  # timer for data loading per iteration\n",
    "    epoch_iter = (\n",
    "        0  # the number of training iterations in current epoch, reset to 0 every epoch\n",
    "    )\n",
    "    visualizer.reset()  # reset the visualizer: make sure it saves the results to HTML at least once every epoch\n",
    "    model.update_learning_rate()  # update learning rates in the beginning of every epoch.\n",
    "    for i, data in enumerate(dataset):  # inner loop within one epoch\n",
    "        iter_start_time = time.time()  # timer for computation per iteration\n",
    "        if total_iters % opt.print_freq == 0:\n",
    "            t_data = iter_start_time - iter_data_time\n",
    "\n",
    "        total_iters += opt.batch_size\n",
    "        epoch_iter += opt.batch_size\n",
    "        model.set_input(data)  # unpack data from dataset and apply preprocessing\n",
    "        model.optimize_parameters()  # calculate loss functions, get gradients, update network weights\n",
    "\n",
    "        if (\n",
    "            total_iters % opt.display_freq == 0\n",
    "        ):  # display images on visdom and save images to a HTML file\n",
    "            save_result = total_iters % opt.update_html_freq == 0\n",
    "            model.compute_visuals()\n",
    "            visualizer.display_current_results(\n",
    "                model.get_current_visuals(), epoch, save_result\n",
    "            )\n",
    "\n",
    "        if (\n",
    "            total_iters % opt.print_freq == 0\n",
    "        ):  # print training losses and save logging information to the disk\n",
    "            losses = model.get_current_losses()\n",
    "            t_comp = (time.time() - iter_start_time) / opt.batch_size\n",
    "            visualizer.print_current_losses(epoch, epoch_iter, losses, t_comp, t_data)\n",
    "            if opt.display_id > 0:\n",
    "                visualizer.plot_current_losses(\n",
    "                    epoch, float(epoch_iter) / dataset_size, losses\n",
    "                )\n",
    "\n",
    "        if (\n",
    "            total_iters % opt.save_latest_freq == 0\n",
    "        ):  # cache our latest model every <save_latest_freq> iterations\n",
    "            print(\n",
    "                \"saving the latest model (epoch %d, total_iters %d)\"\n",
    "                % (epoch, total_iters)\n",
    "            )\n",
    "            save_suffix = \"iter_%d\" % total_iters if opt.save_by_iter else \"latest\"\n",
    "            model.save_networks(save_suffix)\n",
    "\n",
    "        iter_data_time = time.time()\n",
    "    if (\n",
    "        epoch % opt.save_epoch_freq == 0\n",
    "    ):  # cache our model every <save_epoch_freq> epochs\n",
    "        print(\n",
    "            \"saving the model at the end of epoch %d, iters %d\" % (epoch, total_iters)\n",
    "        )\n",
    "        model.save_networks(\"latest\")\n",
    "        model.save_networks(epoch)\n",
    "\n",
    "    print(\"Computing the following metrics for monitoring: PSNR and SSIM\")\n",
    "    model_for_eval = model\n",
    "    model_for_eval.eval()\n",
    "\n",
    "    print(\n",
    "        \"End of epoch %d / %d \\t Time Taken: %d sec\"\n",
    "        % (epoch, opt.n_epochs + opt.n_epochs_decay, time.time() - epoch_start_time)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "inpainting-basic",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
